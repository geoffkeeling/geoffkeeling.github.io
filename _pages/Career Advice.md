---
permalink: /careers-advice/
title: "Careers Advice"
excerpt: "Advice for students and early-career researchers interested in AI ethics and governance."
author_profile: true
---

I have benefitted from some great career advice and nowadays people often ask me how to make a career in Responsible AI and adjacent fields. Here is the best advice I have to offer. It’s frank and to the point. But that’s the kind of advice I find most helpful. The advice is tailored mostly to industry and academic research roles but some core principles are generalisable. The people who will find this advice most useful are those who want to emulate a career like mine, starting out in academia and then transitioning to an industry research scientist position. 

**Table of Contents**
* [1. Caveats](#section-1)
* [2. First Steps](#section-2)
* [3. Finding a Niche](#section-3)
  * [3.1. Understand Skill Arbitrage](#section-3.1)
  * [3.1. Don’t Follow the Crowd, Anticipate It](#section-3.2)
  * [3.1. Leverage Branding](#section-3.3)
* [4. Networking](#section-4)
* [5. Job Applications](#section-5)
* [6. Getting Promoted](#section-6)
* [7. Mental Health](#section-7)

---

## 1. Caveats {: #section-1 }

Take this advice with a grain of salt. For anything that you want to achieve, the best advice on how to achieve that thing will come from people who have achieved that thing countless times. For example, if you want to make a chair, you should probably get advice from a carpenter who has made chairs countless times. Such a carpenter is likely to have a good causal model of how input variables in the construction of chairs relate to the relevant outcome variables. Landing a dream job is a rare event for most people. Hence most people have pretty naff causal models of how to land dream jobs and massively underweight the role of circumstance and sheer luck. I think the best careers advice comes from experienced hiring managers, recruiters, bar raisers, and the like. They have seen it all from the other side of the table and have great causal models with respect to the features of applications and candidates that are conducive to success. In this regard there are many people who are better qualified to give careers advice than I am.

The salient point to look out for is that my advice will be tainted with survivorship bias. I’m not able to reliably discern what was down to my character traits as opposed to me being in the right place at the right time. Still, I think the advice may be helpful to at least some people. The most important lessons I have learned are from things that went very badly wrong as opposed to things that went right. In particular, the character traits that I have now that I am most proud of were forged in massive failures and did not come naturally to me. Two examples: (1) I used to be quite complacent. In high school, for example, I received an offer to do my undergrad at Cambridge and then missed my offer due to not working particularly hard. Now I am extremely conscientious because I understand that a significant part of being successful is hard work. (2) Early in my career I spent way too much time following academic trends and not enough time thinking about what was actually important to work on from a societal point of view. What initially drove me to research was prestige and the result was that I got prestige at the cost of my own satisfaction with the work I was doing. I now work only on things which I think really matter. 

One final caveat is that this advice is not for everyone. Don’t take what I say at face value. Read it carefully, think about it and make up your own mind on the degree to which my advice applies to you and the circumstances in which you find yourself. It’s possible, for example, that you have a different risk tolerance to me or that you value job security more than I do. I suspect that you might get most out of the advice by making precise in what respects my advice fails to apply to your situation. And that’s great. The whole point of writing this stuff down is to help you get to where you want to go, and it doesn’t matter to me whether you use it as an instruction manual or as material to rip the shit out of in order to formulate much better advice for your situation.


---

## 2. First Steps {: #section-2 }

This section is for people who don’t currently work in Responsible AI but would like to. It’s aimed at recent graduates and mid-career folks who have jobs outside of Responsible AI, but some of it might also be helpful for undergrads and PhD students. What I’m trying to do is characterise the set of options available to people who are looking to break into the field. 

There are many different career paths in the field of Responsible AI. They include academia, industry research, ethics consulting, policy and governance, community building, auditing, trust and safety, education, fundraising, legal and compliance, product management, and more. 

Different roles have different requirements. For example, industry research typically requires a PhD and publications in top journals or conferences, whereas community building requires experience organising events or building platforms for people to network with one another. Some roles require a technical skillset. These include many research scientist and research engineer positions in industry, although there are some rare research scientist roles in industry that focus specifically on ethics or social science. Other roles do not require a technical skillset such as policy and governance and community building, although these roles often require a strong conceptual understanding of AI. To get a handle on the different job roles out there and what skills are needed, take a look at the 80,000 Hours and All Tech is Human jobs boards. 

It’s also fine to experiment with different roles. Experience is often more cumulative than people think and the boundaries between distinct kinds of roles are often porous. For example, I started out in academia, became a bioethicist, and then transferred to a research scientist position. One of my collaborators started out in front-end software development, switched to user experience research and now researches machine cognition and consciousness. Another collaborator was a program manager who started running ethics workshops for engineering and product teams, and used that to pivot into an ethics and policy research role. I think it’s fair to say that most people in Responsible AI are making it up as they go along and you should do the same. 

To break into Responsible AI, one option is to apply to roles that are close to your current role but which have a Responsible AI bent, and then work from that higher-leverage platform to get your dream job. For example, if you are a coder and you want to work on ML fairness, maybe apply to be a software engineer on a trust and safety team first, and then use that position as leverage to apply for your ideal role as a research engineer on a dedicated ML fairness team. Another thing that you could do is start working on stuff related to Responsible AI in the role that you currently have (e.g. pilot and then scale an ethics workshop at work), and then leverage that experience when applying to your dream Responsible AI role. The general principle in both cases is that it’s fine to take a multi-step path to the job that you would ideally want to do. 

If you’re feeling a bit underleveraged, i.e. it’s not clear what you can point to on your CV that makes you an obvious candidate for any roles currently being advertised, or your current role does not allow you the flexibility to start random ethics initiatives, it may make sense to do a qualification. I think that the [MSt in Practical Ethics](https://www.ox.ac.uk/admissions/graduate/courses/mst-practical-ethics) at Oxford and the [MSt in AI Ethics and Society](https://www.pace.cam.ac.uk/courses/mst-ai-ethics-and-society) at Cambridge are excellent courses for people who want to pivot. There are also organisations like [Blue Dot Impact](https://bluedot.org/) who run courses to help people break into the field. I’ve also noticed some universities, such as the [London School of Economics](https://www.lse.ac.uk/study-at-lse/executive-education/programmes/ethics-of-ai), offering short non-degree courses in AI ethics, although I’m uncertain how effective these things are for getting jobs.

It may even be worth doing a PhD. Whether or not to do a PhD is an extremely personal decision. They can be quite long and the pay is terrible (if funded) or negative (if unfunded). Even so, PhDs can be extremely rewarding, and (at least in the UK) it’s possible to complete them in three years. Furthermore, doing a PhD is typically the best way to land an industry research scientist position, especially if integrated with internships at tech companies, and it’s also necessary for pursuing an academic pathway (i.e. a postdoc or a tenure track assistant professorship). If you do end up doing a PhD, I’d advise you to hit the ground running, publish extensively and present at high profile conferences. Overall seek to emulate the behaviour of an experienced academic. Merely having a PhD is insufficient for landing an industry research or academic position. It’s not even close. So if you are going to take this route, it’s important that you do so with the explicit intention of using the time to build an extensive research portfolio.

Last, I’d recommend getting some advice from [80,000 Hours](https://80000hours.org/) or getting a mentor through the All Tech is Human [mentorship programme](https://alltechishuman.org/responsible-tech-mentorship-program). Generic advice is only so helpful, and so what’s best is to have some tailored advice from people who have their finger on the pulse of the job market.

---

## 3. Finding a Niche {: #section-3 }


Responsible AI, as a field, is not yet at a level of maturity where career paths are well-defined. The people who work in Responsible AI often carve out niches for themselves and bend the scope of pre-established roles (e.g. product management, consultancy, policy research) in a Responsible AI direction. The point is that you need to be entrepreneurial in planning out a career, although this is liable to change as the field becomes more mature. Here are three pieces of advice for carving out a bespoke career path for yourself within Responsible AI. 

### 3.1. Understand Skill Arbitrage {: #section-3.1 }

You can greatly increase your value on the job market by acquiring cheap skills in one domain and selling them in another domain where those skills are in short supply and high demand.  

For example, basically everyone who studies philosophy at the graduate level can formulate arguments, explain difficult concepts, articulate competing views and assess their costs and benefits, and give a half-decent presentation. Having these skills in academic philosophy is table stakes. The discipline is saturated with people who possess these skills to a high degree. Hence the mere possession of these skills is unlikely to get you very far. On the other hand, the skillset of a reasonably good philosopher is exceptionally rare in industry research and ethics consulting, and these skills are in high demand. Provided you can market your skills in the language of these other fields so that people understand how you satisfy their needs, the scarcity of philosophy skills in these other fields can make you an extremely valuable asset.

An important corollary to this point is that you can massively increase your value on the job market by acquiring rare combinations of skills that are in high demand. For example, the set of people who are competent AI policy specialists and competent software engineers is very small. So, if you are an AI policy specialist you can greatly increase your value on the job market by learning to code at least reasonably well. That puts you in a very rare skills bucket and enables you to engage in high-value work like translating policy ideas into engineering practice and interfacing between policy and engineering teams. My general advice in this regard is to pick a base skillset (e.g. policy) and then acquire 1-2 other skillsets (e.g. coding, specialist knowledge of CBRN risks) such that the combination of skillsets is in high demand and the set of people who share the relevant combination of skills is less than ~10 globally. This strategy requires you to focus on getting bespoke jobs but the payoff in terms of market value is definitely worth it.


### 3.2. Don’t Follow the Crowd, Anticipate It {: #section-3.2 }

You ideally want to do good work on topics that are massively significant, as a lot of good stuff follows from doing good work on such a topic (e.g. funding, citations, job opportunities). Where most people mess up is in chasing trends rather than anticipating them. Fields can very quickly become saturated as everyone rushes to get involved in the next big thing, and running into a saturated field is essentially a career cul-de-sac as the supply of labour exceeds demand. 

What set me up in Responsible AI was doing a PhD on autonomous vehicle ethics between 2017 and 2020. The field was nascent at the time, which allowed me to make a name for myself quickly while also having access to cool opportunities like engaging with policymakers and giving talks at big conferences. (There was also a bit of arbitrage involved here too, as I brought a solid understanding of normative ethics and decision theory to bear on a new field of applied ethics which meant that my research contributions were significantly stronger than the median research in the field.) Becoming a key player in autonomous vehicle ethics was instrumental in getting me a postdoc at Stanford in 2020-21. Stanford provided me with a professional network that made it impossible to not realise that LLMs were the next big thing, so I was able to pivot my research agenda towards LLMs and ultimately LLM-based agents, and ride that wave also.

My mental model here is one of making exponential progress by stacking diminishing returns curves. The first handful of researchers in a field see outsized impact in shaping the trajectory of the field and then the marginal impact of each additional researcher diminishes rapidly. Hence one way to make exponential progress is to catch the steep part of the curve in one emerging field, then use the leverage gained in that field to pivot to another emerging field, and catch the steep part of the curve there also. This process can be repeated indefinitely in a rapidly evolving technological landscape. One big point of caution here, as I mentioned in the Caveats section, is not simply to chase the next big thing, as that will result in meaningless status chasing. In hindsight I think the entire field of autonomous vehicle ethics was largely a waste of time. But the high-impact topics that I’ve worked on more recently like AI welfare and the ethics of AI agents are in my view much more likely to have a positive societal impact over the long run. 

There’s also obviously an element of risk here. Even good forecasters can badly misjudge the direction that things will go in. Three points in response: First, as with investing, diversification is important. You don’t have to go all in on any particular area. In my own career, I’ve invested heavily in some high-risk high-reward areas (e.g. AI welfare), but I’ve done that alongside substantial investment in much safer bets (e.g. ML fairness, ethics of AI agents). I believe that AI welfare will be a massively important public conversation. But I could be mistaken, and if I am, I have maintained active participation in other more conventional literatures that I can lean back into if needed. Second, don’t underestimate your agency in making big things happen. For example, whether or not a literature that you’re contributing to becomes a big deal is in large part within your control. Doing great work will attract great people. Third, with the exception of frontier ML work (in which forecasts are typically based on extrapolating empirical regularities that may or may not continue to hold indefinitely), academic forecasting in Responsible AI is essentially a matter of information arbitrage because most fields lag behind the technological frontier. There is, for example, a 1-3 year time lag between a particular ML development and philosophers starting to discuss that development in the literature, so simply being ahead of the curve with respect to the philosophers and on the curve with respect to ML is usually sufficient. But this time-lag is probably shrinking and will continue to do so as the AI boom continues.


### 3.3. Leverage Branding {: #section-3.2 }

Did I mention that I went to Stanford? I appreciate that this sounds like extremely crass advice, but it is the case that being affiliated with prestigious universities and companies makes a huge difference to your career. I first started thinking about this point when I was doing my PhD at Bristol. For context, Bristol is a good university in the UK, but it is not Oxford or Cambridge. I noticed that the placement record for Bristol’s PhD programme (that is, a list of the places that PhD graduates from the relevant department end up getting jobs) was not great. Many people did not get jobs and those that did get jobs typically ended up at comparable and often lower ranked universities than Bristol. I realised that I needed to get myself in a better reference class before hitting the job market and it seemed like the easiest way to do this was to get a better university affiliation. So, I applied to a research assistant position at the Leverhulme Centre for the Future of Intelligence at the University of Cambridge and worked extremely hard to ensure that I got the position. (That is, I prepped for interviews like my life depended on it.) Fortunately, I got the job and was able to apply to postdocs at Stanford and Oxford with a dual affiliation.

Once I got to Stanford, things felt qualitatively different. There is a massive credibility boost and people take you extremely seriously. I’m not sure that I would have gotten into Google had I not been applying from Stanford (or a relevantly similar university). Minimally, I’m sure it helped a lot. Being at Google has a similar effect. Once again, I don’t like the fact that this is true, but I think one of the best pieces of career advice I can give is to barnacle yourself to a high-status institution. (For what it’s worth, there are also hacky ways to do this like getting some kind of affiliate or honorary position at a prestigious university, which may be high-status to people outside academia but academics will typically give minimal credibility to these positions.)

I also want to register that these big-name universities and companies can seem out of reach. It’s extremely difficult to get into Cambridge, Stanford or Google, and I don’t want to trivialise how much effort is required to make these leaps. Much of what I say in the Job Applications section helps to demystify the process of breaking into these top-tier institutions. Briefly, the most important things are to be targeted in your applications and outwork the competition. 


---

## 4. Networking {: #section-4 }

I am autistic and therefore frightened of people. Hence networking is not something that I am particularly good at and not something that I have invested a huge amount of time doing. But I think my aversion to networking has helped me make a stark assessment of the cost-benefit profile of networking with respect to career growth. If I thought it was more important, I would hold my nose and do more of it. But, as it stands, I don’t think it’s that important. What follows are some points which try to unpack and contextualise this assessment. The advice here is highly specific to research careers in industry and academia and may not generalise to network-heavy career paths like community building, advocacy and ethics consulting.

### 4.1. Everyone Networks

### 4.2. Opportunity Costs

### 4.3. Opportunism

### 4.4. Second-Order Networking

### 4.5. Extracting Value

### 4.6. Find a Good Job

---

## 5. Job Applications {: #section-5 }
Placeholder text for Section 5. Finally, this section offers advice on navigating the job market, from crafting your CV to preparing for interviews. We will cover common interview questions and strategies for effectively communicating your research and experience to potential employers.

---

## 6. Getting Promoted {: #section-6 }
Placeholder text for Section 5. Finally, this section offers advice on navigating the job market, from crafting your CV to preparing for interviews. We will cover common interview questions and strategies for effectively communicating your research and experience to potential employers.

---

## 7. Mental Health {: #section-7 }
Placeholder text for Section 5. Finally, this section offers advice on navigating the job market, from crafting your CV to preparing for interviews. We will cover common interview questions and strategies for effectively communicating your research and experience to potential employers.
